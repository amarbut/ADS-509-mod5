{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2cfb05",
   "metadata": {},
   "source": [
    "# ADS-509 Assignment 5.1\n",
    "\n",
    "## Finetuning LLMs\n",
    "**Student Version**\n",
    "\n",
    "In this assignment, you will use a small, locally-hosted LLM (`google/flan-t5-small`) to evaluate performance on the SST‑2 sentiment classification benchmarking dataset. You will compare how the same model performs after:\n",
    "1) Zero‑shot prompting\n",
    "2) Few‑shot prompting\n",
    "3) Fine‑tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5de66-145e-41fe-9831-94a2be02d478",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it.\n",
    "\n",
    "Work through this notebook as if it were a worksheet, completing the code sections marked with **TODO** in the cells provided. Similarly, written questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you to fill in with your answers. **Make sure to answer every question marked with a Q: for full credit**.\n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential import statements and make sure that all such statements are moved into the designated cell.\n",
    "\n",
    "A .pdf of this notebook, with your completed code and written answers, is what you should submit in Canvas for full credit. **DO NOT SUBMIT A NEW NOTEBOOK FILE OR A RAW .PY FILE**. Submitting in a different format makes it difficult to grade your work, and students who have done this in the past inevitably miss some of the required work or written questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033825e1",
   "metadata": {},
   "source": [
    "## Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da941f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import datasets, transformers, evaluate, torch  # type: ignore\n",
    "except Exception:\n",
    "    %pip install -q datasets transformers evaluate accelerate sentencepiece\n",
    "\n",
    "import os, random, numpy as np, warnings\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc988b0e",
   "metadata": {},
   "source": [
    "## Load Dataset and Model\n",
    "\n",
    "For this assignment, you will be comparing performance on a common language model benchmarking task: predicting the sentiment for the Stanford Sentiment Treebank ([SST-2](https://nlp.stanford.edu/sentiment/index.html)). We will use the same model, [Flan-T5-Small](https://huggingface.co/google/flan-t5-small), across all of our \"training\" methods so that the results are directly comparable.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "- Use your preferred method to select a sample of 2000 sentences from the train dataset.\n",
    "\n",
    "**Q**: After reading a little bit about the Sentiment Treebank project at the link above, and recognizing that the paper was written in 2013, what method do we now use to provide the same kind of benefit that they intended with their tree-based sentiment representations?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b726314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw = load_dataset('glue', 'sst2')\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9665be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['train'][5], raw['validation'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: select a sample for your training dataset\n",
    "\n",
    "train_size = 2000\n",
    "train_ds = ??\n",
    "label_names = train_ds.features['label'].names\n",
    "eval_ds  = raw['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "MODEL_NAME = 'google/flan-t5-small'\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "flan = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016af13",
   "metadata": {},
   "source": [
    "## Zero‑Shot and Few‑Shot Prompting\n",
    "\n",
    "One of the major benefits of today's generative models is that they can often be used effectively with no supervised, task-specific training (i.e. fine tuning). This avoids the time and expense needed to compile and train on a labeled dataset and is called zero-shot or few-shot prompting. However, using a generative model makes performance evaluation more complex, since the set of possible outputs is not pre-defined (i.e. the model can potentially produce any tokens in its vocabulary).\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "- Define a function to format zero-shot prompts. *HINT: Your prompt should introduce the labeling task (without providing an example), and end with an indication that it should respond.*\n",
    "- Define a function to format few-shot prompts using the examples provided. *HINT: This should be very similar to the zero-shot format, but with a couple example input/output provided.*\n",
    "- Define a function that will normalize the generated output for evaluation.\n",
    "\n",
    "**Q**: Discuss one additional benefit and drawback to using a generative model with zero-shot or few-shot prompting in place of traditional supervised learning methods.\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOTS = [\n",
    "    ('This movie was fantastic and heartwarming.', 'positive'),\n",
    "    ('The plot was boring and predictable.', 'negative'),\n",
    "]\n",
    "\n",
    "def zshot_prompt(text):\n",
    "    # TODO: format your input text into a zero-shot prompt\n",
    "    return ??\n",
    "\n",
    "def fshot_prompt(text):\n",
    "    # TODO: format your input text into a few-shot prompt, using the examples above\n",
    "    return ??\n",
    "\n",
    "def norm_label(s: str):\n",
    "    s = (s or '').lower()\n",
    "    # TODO: normalize the generated output to produce the labels 'positive' or 'negative'\n",
    "    if ??: \n",
    "        return 'positive'\n",
    "    if ??: \n",
    "        return 'negative'\n",
    "    return s.strip()\n",
    "\n",
    "@torch.no_grad() #tells pytorch not to store any gradients\n",
    "def flan_predict(texts, mode='zero', max_new_tokens=3):\n",
    "    preds = []\n",
    "    for t in texts:\n",
    "        prompt = zshot_prompt(t) if mode == 'zero' else fshot_prompt(t)\n",
    "        inputs = tok(prompt, return_tensors='pt')\n",
    "        outputs = flan.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        out = tok.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        preds.append(norm_label(out))\n",
    "    return preds\n",
    "\n",
    "print('Zero-shot:', flan_predict(['I loved this movie', 'This was terrible'], mode='zero'))\n",
    "print('Few-shot :', flan_predict(['I loved this movie', 'This was terrible'], mode='few'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd17150",
   "metadata": {},
   "source": [
    "#### Evaluate Prompting Methods\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "- Define a function (or use a pre-existing implementation) that computes accuracy, with lists of labels and predictions as input.\n",
    "- Use your `flan_predict` function from above to produce zero-shot and few-shot predictions over your evaluation data.\n",
    "\n",
    "**Q**: Reflect on the performance of these two methods. Was there anything that surprised you?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cef589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    # TODO: compute the accuracy of your predictions\n",
    "    return ??\n",
    "\n",
    "def label_to_str(y): # the SST-2 dataset stores the sentiment labels as integers\n",
    "    return label_names[int(y)]\n",
    "\n",
    "eval_texts = [ex['sentence'] for ex in eval_ds]\n",
    "eval_labels = [label_to_str(ex['label']) for ex in eval_ds]\n",
    "\n",
    "# TODO: predict the sentiment of your evaluation dataset with the zero-shot and few-shot prompting methods\n",
    "z_preds = ??\n",
    "f_preds = ??\n",
    "\n",
    "z_acc = accuracy(eval_labels,z_preds)\n",
    "f_acc = accuracy(eval_labels,f_preds)\n",
    "print({'zero_shot_acc': round(z_acc, 4), 'few_shot_acc': round(f_acc, 4)})\n",
    "print(\"Zero-Shot Confusion Matrix:\\n\", confusion_matrix(eval_labels,z_preds))\n",
    "print(\"Few-Shot Confusion Matrix:\\n\", confusion_matrix(eval_labels,f_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652266e",
   "metadata": {},
   "source": [
    "## Model Fine Tuning\n",
    "\n",
    "Now we will fine tune the same Flan-T5 model on the SST-2 training dataset and compare performance. Since we are still using a generative model, the data needs to be formatted to support generation as we did above. We also need to define a custom metric function for the model to use during training, ensuring that the output matches our expected labels for evaluation.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "- Define a function to format data to use in a generative model training pipeline.\n",
    "- Use your `norm_label` function from above to process the model output for evaluation.\n",
    "\n",
    "**Q**: How would the model training pipeline change if we were using a representative model (i.e. an encoder-side tranformer model like BERT) instead of a generative model? Which type of model makes more sense when doing a supervised training task?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87991fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(text):\n",
    "    # TODO: format the model input to support generation\n",
    "    inp = ??\n",
    "    tgt = label_to_str(text['label'])\n",
    "    return {'input_text': inp, 'target_text': tgt}\n",
    "\n",
    "train_formatted = train_ds.map(format_data) # we can use the .map() function in place of looping over the whole dataset\n",
    "eval_formatted  = eval_ds.map(format_data)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    model_inputs = tok(batch['input_text'], truncation=True) # convert text to token ids and cut off very long inputs\n",
    "    with tok.as_target_tokenizer():\n",
    "        labels = tok(batch['target_text'], truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "train_toks = train_formatted.map(tokenize_fn, batched=True, remove_columns=train_formatted.column_names)\n",
    "eval_toks  = eval_formatted.map(tokenize_fn,  batched=True, remove_columns=eval_formatted.column_names)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tok, model=flan) # the collator handles data batching during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    pred_texts = tok.batch_decode(preds, skip_special_tokens=True) # translate token_ids to text\n",
    "    labels_clean = []\n",
    "    for row in labels:\n",
    "        row = [id for id in row if id != -100] # skip padding tokens\n",
    "        labels_clean.append(row)\n",
    "    ref_texts = tok.batch_decode(labels_clean, skip_special_tokens=True) # translate token_ids to text\n",
    "    \n",
    "    # TODO: normalize the model outputs for evaluation\n",
    "    preds_norm = ??\n",
    "    return {'accuracy': accuracy(ref_texts,preds_norm)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='outputs/flan_t5_sst2',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=flan,\n",
    "    args=training_args,\n",
    "    train_dataset=train_toks,\n",
    "    eval_dataset=eval_toks,\n",
    "    tokenizer=tok,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_out = trainer.predict(eval_toks)\n",
    "ft_acc = round(eval_out.metrics[\"test_accuracy\"], 4)\n",
    "print({'finetuned_flan_t5_eval_accuracy': ft_acc})\n",
    "# Decode predicted sequences\n",
    "pred_texts = tok.batch_decode(eval_out.predictions, skip_special_tokens=True)\n",
    "\n",
    "# Decode reference (true) sequences, removing padding (-100)\n",
    "labels_clean = [[id for id in row if id != -100] for row in eval_out.label_ids]\n",
    "ref_texts = tok.batch_decode(labels_clean, skip_special_tokens=True)\n",
    "print(confusion_matrix(ref_texts, pred_texts, labels=[\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202aaaec",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "**Q**: Reflect on the performance of your three methods. Is there anything that was surprising? Would you do anything to improve the performance of any of the methods? Are there any other methods that you would like to compare?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1159e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'zero_shot_flan_t5_acc': round(z_acc, 4),\n",
    "    'few_shot_flan_t5_acc': round(f_acc, 4),\n",
    "    'finetuned_flan_t5_acc' : ft_acc,\n",
    "}\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
